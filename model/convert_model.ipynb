{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c4503a-7c2f-49c2-85b8-71dee471f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e447b45-51a9-44e2-8b28-c991f5745e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 15:29:01.442582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 15:29:04.932159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:29:04.932192: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-09-07 15:29:17.394028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:29:17.396211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:29:17.396220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Activation\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ece70b4-c14f-4857-ace8-97d7ed809440",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = ['input', 'inputlayer']\n",
    "ACTIVATIONS = ['relu', 'linear', 'leakyrelu', 'sigmoid', 'gelu']\n",
    "SUPPORTED_LAYERS = ['dense', 'dropout', 'batchnormalization'] + ACTIVATIONS + INPUT\n",
    "\n",
    "def txt_to_h5(weights_file_name, output_file_name=''):\n",
    "    '''\n",
    "    Convert a txt file to Keras h5 file\n",
    "    REQUIRED:\n",
    "        weights_file_name (str): path to a txt file used by neural fortran\n",
    "    OPTIONAL:\n",
    "        output_file_name  (str): desired output path for the produced h5 file\n",
    "    '''\n",
    "\n",
    "    lr               = False\n",
    "    bias             = []                                       # dense layer\n",
    "    weights          = []                                       # dense layer\n",
    "    batchnorm_params = []                                       # batchnormalization layers\n",
    "\n",
    "    bias_count       = 0\n",
    "    weights_count    = 0\n",
    "    batchnorm_count  = 0\n",
    "\n",
    "    with open(weights_file_name, mode='r') as weights_file:\n",
    "        lines = weights_file.readlines()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "\n",
    "            if idx == 0:\n",
    "                num_layers = int(line)\n",
    "                continue\n",
    "\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            layer_type = line[0]\n",
    "\n",
    "            if layer_type in SUPPORTED_LAYERS:\n",
    "                param = line[1]\n",
    "                if layer_type == 'input':\n",
    "                    input = Input(shape=(int(param),), name = \"input\")\n",
    "                    x     = input\n",
    "\n",
    "                elif layer_type == 'dense':\n",
    "                    bias_count += 1; weights_count += 1\n",
    "                    x = Dense(int(param),name = \"dense_{}\".format(weights_count))(x)\n",
    "\n",
    "                elif layer_type == 'dropout':\n",
    "                    x = Dropout(float(param))(x)\n",
    "\n",
    "                elif layer_type == 'relu':\n",
    "                    x = Activation('relu')(x)\n",
    "\n",
    "                elif layer_type == 'relu':\n",
    "                    x = Activation('relu',alpha=float(param))(x)\n",
    "\n",
    "                elif layer_type == 'batchnormalization':\n",
    "                    batchnorm_count += 4\n",
    "                    x = BatchNormalization(name='batch_normalization_{}'.format(batchnorm_count // 4))(x)\n",
    "                elif layer_type == 'linear':\n",
    "                    x = Activation('linear')(x)\n",
    "\n",
    "            elif not layer_type.isalpha():\n",
    "                if lr == False:\n",
    "                    lr = float(line[0]); continue\n",
    "\n",
    "                # found bias or weights numbers\n",
    "                w = np.asarray([float(num) for num in line])\n",
    "\n",
    "                if bias_count > 0:\n",
    "                    bias_count -= 1\n",
    "                    bias.append(w)\n",
    "\n",
    "                elif weights_count > 0:\n",
    "                    weights_count -= 1\n",
    "                    weights.append(w)\n",
    "\n",
    "                elif batchnorm_count > 0:\n",
    "                    batchnorm_count -= 1\n",
    "                    batchnorm_params.append(w)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizers.SGD(lr),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # set weights and biases\n",
    "    for idx, w in enumerate(weights):\n",
    "        name    = 'dense_{}'.format(idx+1)\n",
    "        layer   = model.get_layer(name)\n",
    "        w       = w.reshape(layer.output_shape[1],layer.input_shape[1]).T\n",
    "        layer.set_weights( [w, bias[idx]] )\n",
    "\n",
    "    # set batchnorm parameters\n",
    "    for idx in range(0,len(batchnorm_params),4):\n",
    "        params  = batchnorm_params[idx:idx+4]\n",
    "        name    = 'batch_normalization_{}'.format(idx // 4 + 1)\n",
    "        layer   = model.get_layer(name)\n",
    "        layer.set_weights([\n",
    "            params[1],\n",
    "            params[0],\n",
    "            params[2],\n",
    "            params[3],\n",
    "        ])\n",
    "\n",
    "    # view summary\n",
    "\n",
    "    if not output_file_name:\n",
    "        # if not specified will use path of weights_file with h5 extension\n",
    "        output_file_name = weights_file_name.replace('.txt', '_converted.h5')\n",
    "\n",
    "    model.save(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1badbcd5-6993-49bf-ae0b-d33cf2662209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_to_txt(weights_file_name, output_file_name=''):\n",
    "    '''\n",
    "    Convert a Keras h5 file to a txt file\n",
    "    REQUIRED:\n",
    "        weights_file_name (str): path to a Keras h5 file\n",
    "    OPTIONAL:\n",
    "        output_file_name  (str): desired path for the produced txt file\n",
    "    '''\n",
    "\n",
    "    info_str         = '{name}\\t{info}\\n'                       # to store in layer info; config of network\n",
    "    bias             = []                                       # dense layer\n",
    "    weights          = []                                       # dense layer\n",
    "    layer_info       = []                                       # all layers\n",
    "    batchnorm_params = []                                       # batchnormalization layers\n",
    "    out_bias_dict    = {}                                       # output bias if multiple dense outputs\n",
    "    out_weights_dict = {}                                       # output weights if multiple dense outputs\n",
    "    out_bias         = []                                       # output bias if multiple dense outputs\n",
    "    out_weights      = []                                       # output weights if multiple dense outputs\n",
    "    #check and open file\n",
    "    with h5py.File(weights_file_name,'r') as weights_file:\n",
    "\n",
    "        # weights of model\n",
    "        model_weights = weights_file['model_weights']\n",
    "        keras_version = weights_file.attrs['keras_version']\n",
    "\n",
    "        if 'training_config' in weights_file.attrs:\n",
    "            training_config = weights_file.attrs['training_config']#.decode('utf-8')\n",
    "            training_config = training_config.replace('true','True')\n",
    "            training_config = training_config.replace('false','False')\n",
    "            training_config = training_config.replace('null','None')\n",
    "            training_config = eval(training_config)\n",
    "\n",
    "            if 'learning_rate' in training_config['optimizer_config']['config']: learning_rate = training_config['optimizer_config']['config']['learning_rate']\n",
    "            else: learning_rate = training_config['optimizer_config']['config']['lr']\n",
    "        else:\n",
    "            warnings.warn('Model has not been compiled: Setting learning rate default')\n",
    "            learning_rate = 0.001\n",
    "\n",
    "        # Decode using the utf-8 encoding; change values for eval\n",
    "        model_config = weights_file.attrs['model_config']#.decode('utf-8')\n",
    "        model_config = model_config.replace('true','True')\n",
    "        model_config = model_config.replace('false','False')\n",
    "        model_config = model_config.replace('null','None')\n",
    "        # convert to dictionary\n",
    "        model_config = eval(model_config)\n",
    "\n",
    "        # store first dimension for the input layer\n",
    "        layer_info.append(\n",
    "            info_str.format(\n",
    "                name = 'input',\n",
    "                info = model_config['config']['layers'][0]['config']['batch_input_shape'][1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # check what type of keras model sequential or functional\n",
    "        if model_config['class_name'] == 'Model':\n",
    "            layer_config = model_config['config']['layers'][1:]\n",
    "\n",
    "            # get names of input layers and output layers\n",
    "            if model_config['config'].get('output_layers'):\n",
    "                output_layers = model_config['config'].get('output_layers',[])\n",
    "                output_names = [layer[0] for layer in output_layers]\n",
    "\n",
    "            if model_config['config'].get('input_layers'):\n",
    "                input_layers = model_config['config'].get('input_layers',[])\n",
    "                input_names = [layer[0] for layer in input_layers]\n",
    "\n",
    "        else:\n",
    "            layer_config = model_config['config']['layers']\n",
    "\n",
    "        for idx,layer in enumerate(layer_config):\n",
    "            name       = layer['config']['name']\n",
    "            class_name = layer['class_name'].lower()\n",
    "\n",
    "            if class_name not in SUPPORTED_LAYERS:\n",
    "                warning_str = 'Unsupported layer, %s, found! Skipping...' % class_name\n",
    "                warnings.warn(warning_str)\n",
    "                continue\n",
    "            elif class_name == 'dense':\n",
    "                # get weights and biases out of dictionary\n",
    "                layer_weights = np.array(\n",
    "                    model_weights[name][name]['kernel:0']\n",
    "                )\n",
    "\n",
    "                if 'bias:0' in model_weights[name][name]:\n",
    "                    layer_bias = np.array(\n",
    "                        model_weights[name][name]['bias:0']\n",
    "                    )\n",
    "                else:\n",
    "                    warnings.warn('No bias found: Replacing with zeros')\n",
    "                    layer_bias = np.zeros(layer_weights.shape[1])\n",
    "\n",
    "                # store bias values\n",
    "                bias.append(layer_bias)\n",
    "                # store weight value\n",
    "                weights.append(layer_weights)\n",
    "\n",
    "                activation = layer['config']['activation']\n",
    "\n",
    "                if activation not in ACTIVATIONS:\n",
    "                    warning_str = 'Unsupported activation, %s, found! Replacing with Linear.' % activation\n",
    "                    warnings.warn(warning_str)\n",
    "                    activation = 'linear'\n",
    "\n",
    "                # store dimension of hidden dim\n",
    "                layer_info.append(\n",
    "                    info_str.format(\n",
    "                        name = class_name,\n",
    "                        info = layer_weights.shape[1]\n",
    "                    )\n",
    "                )\n",
    "                # add information about the activation\n",
    "                layer_info.append(\n",
    "                    info_str.format(\n",
    "                        name = activation,\n",
    "                        info = 0\n",
    "                    )\n",
    "                )\n",
    "            elif class_name == 'batchnormalization':\n",
    "                # get beta, gamma, moving_mean, moving_variance from dictionary\n",
    "                for key in sorted(model_weights[name][name].keys()):\n",
    "                    # store batchnorm params\n",
    "                    batchnorm_params.append(\n",
    "                        np.array(\n",
    "                            model_weights[name][name][key]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # store batchnorm layer info\n",
    "                layer_info.append(\n",
    "                    info_str.format(\n",
    "                        name = class_name,\n",
    "                        info = 0\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif class_name == 'dropout':\n",
    "                layer_info.append(\n",
    "                    info_str.format(\n",
    "                        name = class_name,\n",
    "                        info = layer['config']['rate']\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif class_name in ACTIVATIONS:\n",
    "                # replace previous dense layer with the advanced activation function (LeakyReLU)\n",
    "                layer_info[-1] = info_str.format(\n",
    "                    name = class_name,\n",
    "                    info = layer['config']['alpha']\n",
    "                )\n",
    "\n",
    "            # if there are multiple outputs, remove what was just added\n",
    "            # and combine in single output\n",
    "            if 'output_names' in locals() and len(output_names) > 1 and name in output_names:\n",
    "                try:\n",
    "                    if class_name not in ['dense']:\n",
    "                        warnings.warn('Only multiple dense outputs allowed! Skipping...')\n",
    "                        continue\n",
    "                    # remove last bias and weights\n",
    "                    # start building up the combined output bias and weights\n",
    "                    out_bias_dict[name]     = bias.pop()\n",
    "                    out_weights_dict[name]  = weights.pop()\n",
    "                    layer_info, out_info    = layer_info[:-2], layer_info[-2:]\n",
    "                except:\n",
    "                    pass\n",
    "    if 'output_names' in locals() and len(output_names) > 1:\n",
    "        #need to combine outputs here into one layer\n",
    "        out_info[0] = out_info[0].replace('1',str(len(output_names)))\n",
    "        layer_info.extend(out_info)\n",
    "\n",
    "        for name in output_names:\n",
    "            out_bias.extend(out_bias_dict.get(name))\n",
    "            out_weights.append(out_weights_dict.get(name))\n",
    "\n",
    "        bias.append(\n",
    "            np.array(out_bias).squeeze()\n",
    "        )\n",
    "        weights.append(\n",
    "            np.array(out_weights).squeeze().T\n",
    "        )\n",
    "\n",
    "    if not output_file_name:\n",
    "        # if not specified will use path of weights_file with txt extension\n",
    "        output_file_name = weights_file_name.replace('.h5', '.txt')\n",
    "\n",
    "    with open(output_file_name,\"w\") as output_file:\n",
    "        output_file.write(str(len(layer_info)) + '\\n')\n",
    "\n",
    "        output_file.write(\n",
    "            ''.join(layer_info)\n",
    "        )\n",
    "\n",
    "        output_file.write(\n",
    "            str(learning_rate) + '\\n'\n",
    "        )\n",
    "\n",
    "        for b in bias:\n",
    "            bias_str = '\\t'.join(\n",
    "                '{:0.7e}'.format(num) for num in b.tolist()\n",
    "            )\n",
    "            output_file.write(bias_str + '\\n')\n",
    "\n",
    "        for w in weights:\n",
    "            weights_str = '\\t'.join(\n",
    "                '{:0.7e}'.format(num) for num in w.T.flatten()\n",
    "            )\n",
    "            output_file.write(weights_str + '\\n')\n",
    "\n",
    "        for b in batchnorm_params:\n",
    "            param_str = '\\t'.join(\n",
    "                '{:0.7e}'.format(num) for num in b.tolist()\n",
    "            )\n",
    "            output_file.write(param_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df10f9f9-9b83-488f-8c4d-1453c9f96254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5_to_txt('model_17-08-2023.h5', output_file_name='mie_quantile_17-08-2023.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
